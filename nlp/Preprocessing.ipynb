{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6e7d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "271befa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\") #en means english language and by default it gives us tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facdcbc7",
   "metadata": {},
   "source": [
    "Go through to different language models \n",
    "https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b30d77",
   "metadata": {},
   "source": [
    "# word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4fdff0",
   "metadata": {},
   "source": [
    "![](spacy_en.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79488e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suresh\n",
      "Kumar\n",
      "Raina\n",
      "was\n",
      "born\n",
      "in\n",
      "Muradnagar\n",
      "of\n",
      "Uttar\n",
      "Pradesh\n",
      "on\n",
      "27\n",
      "November\n",
      "1986\n",
      "into\n",
      "a\n",
      "Kashmiri\n",
      "Pandit\n",
      "family\n",
      ",\n",
      "to\n",
      "parents\n",
      "from\n",
      "Rainawari\n",
      ",\n",
      "Srinagar\n",
      "district\n",
      "of\n",
      "Jammu\n",
      "and\n",
      "Kashmir.[15][16\n",
      "]\n",
      "Raina\n",
      "lives\n",
      "in\n",
      "the\n",
      "Rajnagar\n",
      "neighborhood\n",
      "of\n",
      "Ghaziabad\n",
      "city\n",
      ".\n",
      "He\n",
      "has\n",
      "an\n",
      "older\n",
      "brother\n",
      ",\n",
      "Dinesh\n",
      "Raina.[17\n",
      "]\n",
      "He\n",
      "studied\n",
      "in\n",
      "a\n",
      "boarding\n",
      "school\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Suresh Kumar Raina was born in Muradnagar of Uttar Pradesh on 27 November 1986 into a Kashmiri Pandit family, to parents from Rainawari, Srinagar district of Jammu and Kashmir.[15][16] Raina lives in the Rajnagar neighborhood of Ghaziabad city. He has an older brother, Dinesh Raina.[17] He studied in a boarding school\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5c1e14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suresh"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23e8bc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0802a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c1d82a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfc6aa45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[2:7]\n",
    "type(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "815564e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token0 = doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02c55b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "663c01c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "685b8cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token11 =doc[11]\n",
    "token11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7c1c0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token11.is_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c8e5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suresh ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Kumar ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Raina ==> index:  2 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "was ==> index:  3 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "born ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "in ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Muradnagar ==> index:  6 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "of ==> index:  7 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Uttar ==> index:  8 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Pradesh ==> index:  9 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "on ==> index:  10 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "27 ==> index:  11 is_alpha: False is_punct: False like_num: True is_currency: False\n",
      "November ==> index:  12 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "1986 ==> index:  13 is_alpha: False is_punct: False like_num: True is_currency: False\n",
      "into ==> index:  14 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "a ==> index:  15 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Kashmiri ==> index:  16 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Pandit ==> index:  17 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "family ==> index:  18 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ", ==> index:  19 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "to ==> index:  20 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "parents ==> index:  21 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "from ==> index:  22 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Rainawari ==> index:  23 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ", ==> index:  24 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "Srinagar ==> index:  25 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "district ==> index:  26 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "of ==> index:  27 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Jammu ==> index:  28 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "and ==> index:  29 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Kashmir.[15][16 ==> index:  30 is_alpha: False is_punct: False like_num: False is_currency: False\n",
      "] ==> index:  31 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "Raina ==> index:  32 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "lives ==> index:  33 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "in ==> index:  34 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "the ==> index:  35 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Rajnagar ==> index:  36 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "neighborhood ==> index:  37 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "of ==> index:  38 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Ghaziabad ==> index:  39 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "city ==> index:  40 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  41 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "He ==> index:  42 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "has ==> index:  43 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "an ==> index:  44 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "older ==> index:  45 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "brother ==> index:  46 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ", ==> index:  47 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "Dinesh ==> index:  48 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Raina.[17 ==> index:  49 is_alpha: False is_punct: False like_num: False is_currency: False\n",
      "] ==> index:  50 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "He ==> index:  51 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "studied ==> index:  52 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "in ==> index:  53 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "a ==> index:  54 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "boarding ==> index:  55 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "school ==> index:  56 is_alpha: True is_punct: False like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc404d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('student.txt') as f:\n",
    "    text = f.readlines()\n",
    "    \n",
    "text    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "715a0c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n==================================================\\n\\nName\\tbirth day   \\temail\\n-----\\t------------\\t------\\nVirat   5 June, 1882    virat@kohli.com\\nMaria\\t12 April, 2001  maria@sharapova.com\\nSerena  24 June, 1998   serena@williams.com \\nJoe      1 May, 1997    joe@root.com\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =''.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f77c392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(text)\n",
    "emails =[]\n",
    "\n",
    "for token in doc1:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "        \n",
    "        \n",
    "emails\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e304e",
   "metadata": {},
   "source": [
    "https://spacy.io/api/attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27647482",
   "metadata": {},
   "source": [
    "# Support in other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c03e8968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भैया False\n",
      "जी False\n",
      "! False\n",
      "5000 False\n",
      "₹ True\n",
      "उधार False\n",
      "थे False\n",
      "वो False\n",
      "वापस False\n",
      "देदो False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"hi\")\n",
    "doc = nlp(\"भैया जी! 5000 ₹ उधार थे वो वापस देदो\")\n",
    "for token in doc:\n",
    "    print(token, token.is_currency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47aa6d2",
   "metadata": {},
   "source": [
    "# Customizing tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "412dd01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05cca0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"},\n",
    "])\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1628ca",
   "metadata": {},
   "source": [
    "# Sentence Tokenization or Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4fcddaa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentence)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\tokens\\doc.pyx:923\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42c63c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3713fc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1ef49459190>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4cbdded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f74eaba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d4ae9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "data_websites = [token.text for token in doc if token.like_url ] \n",
    "data_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f128de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab282c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text, doc[token.i+1].text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2f0ea",
   "metadata": {},
   "source": [
    "# Spacy Language Processing Pipelines Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bad0d2",
   "metadata": {},
   "source": [
    "Download trained pipeline\n",
    "To download trained pipeline use a command such as,\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "This downloads the small (sm) pipeline for english language\n",
    "\n",
    "Further instructions on : https://spacy.io/usage/models#quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c57db",
   "metadata": {},
   "source": [
    "![](UGZ3GBW.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "44a9e1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dcd2ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  proper noun  |  Captain\n",
      "america  |  proper noun  |  america\n",
      "ate  |  verb  |  eat\n",
      "100  |  numeral  |  100\n",
      "$  |  numeral  |  $\n",
      "of  |  adposition  |  of\n",
      "samosa  |  proper noun  |  samosa\n",
      ".  |  punctuation  |  .\n",
      "Then  |  adverb  |  then\n",
      "he  |  pronoun  |  he\n",
      "said  |  verb  |  say\n",
      "I  |  pronoun  |  I\n",
      "can  |  auxiliary  |  can\n",
      "do  |  verb  |  do\n",
      "this  |  pronoun  |  this\n",
      "all  |  determiner  |  all\n",
      "day  |  noun  |  day\n",
      ".  |  punctuation  |  .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1037da1",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed9c847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e0ee98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80b3b1",
   "metadata": {},
   "source": [
    "python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15a75baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9e9de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aefcb46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  PROPN  |  Tesla\n",
      "Inc  |  PROPN  |  Inc\n",
      "va  |  VERB  |  aller\n",
      "racheter  |  VERB  |  racheter\n",
      "Twitter  |  VERB  |  twitter\n",
      "pour  |  ADP  |  pour\n",
      "$  |  NOUN  |  dollar\n",
      "45  |  NUM  |  45\n",
      "milliards  |  NOUN  |  milliard\n",
      "de  |  ADP  |  de\n",
      "dollars  |  NOUN  |  dollar\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2d52b",
   "metadata": {},
   "source": [
    "\n",
    "# Adding a component to a blank pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3697b4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cce4a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a66e34",
   "metadata": {},
   "source": [
    "In below image you can see sentecizer component in the pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef957c74",
   "metadata": {},
   "source": [
    "![](sentecizer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b221a18",
   "metadata": {},
   "source": [
    "# Stemming in NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "669c6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2686acd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b379b",
   "metadata": {},
   "source": [
    "# Lemmatization in Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "34b6f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cbe78a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n",
      "better  |  well\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807b9a6",
   "metadata": {},
   "source": [
    "# Customizing lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "75579424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9ee31955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf969683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brah"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8691fded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brother'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6].lemma_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6758bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running | run\n",
      "painting | paint\n",
      "walking | walk\n",
      "dressing | dress\n",
      "likely | like\n",
      "children | children\n",
      "whom | whom\n",
      "good | good\n",
      "ate | ate\n",
      "fishing | fish\n"
     ]
    }
   ],
   "source": [
    "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']\n",
    "for word in lst_words:\n",
    "    print(f\"{word} | {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "841ef534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  |  run\n",
      "painting  |  paint\n",
      "walking  |  walk\n",
      "dressing  |  dress\n",
      "likely  |  likely\n",
      "children  |  child\n",
      "who  |  who\n",
      "good  |  good\n",
      "ate  |  eat\n",
      "fishing  |  fishing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")\n",
    "#using lemmatization in spacy\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6be22ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce15824",
   "metadata": {},
   "source": [
    "# POS tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1ea0bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon  |  PROPN  |  proper noun\n",
      "flew  |  VERB  |  verb\n",
      "to  |  ADP  |  adposition\n",
      "mars  |  NOUN  |  noun\n",
      "yesterday  |  NOUN  |  noun\n",
      ".  |  PUNCT  |  punctuation\n",
      "He  |  PRON  |  pronoun\n",
      "carried  |  VERB  |  verb\n",
      "biryani  |  ADJ  |  adjective\n",
      "masala  |  NOUN  |  noun\n",
      "with  |  ADP  |  adposition\n",
      "him  |  PRON  |  pronoun\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a797eb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection\n",
      "!  |  PUNCT  |  punctuation\n",
      "Dr.  |  PROPN  |  proper noun\n",
      "Strange  |  PROPN  |  proper noun\n",
      "made  |  VERB  |  verb\n",
      "265  |  NUM  |  numeral\n",
      "million  |  NUM  |  numeral\n",
      "$  |  NUM  |  numeral\n",
      "on  |  ADP  |  adposition\n",
      "the  |  DET  |  determiner\n",
      "very  |  ADV  |  adverb\n",
      "first  |  ADJ  |  adjective\n",
      "day  |  NOUN  |  noun\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70707aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection  |  UH  |  interjection\n",
      "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
      "Dr.  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "Strange  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "made  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
      "265  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "million  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "$  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "on  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
      "the  |  DET  |  determiner  |  DT  |  determiner\n",
      "very  |  ADV  |  adverb  |  RB  |  adverb\n",
      "first  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "day  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3b62baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quits | VBZ | verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He quits the job\")\n",
    "\n",
    "print(doc[1].text, \"|\", doc[1].tag_, \"|\", spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8fe4c90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit | VBD | verb, past tense\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"he quit the job\")\n",
    "\n",
    "print(doc[1].text, \"|\", doc[1].tag_, \"|\", spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6599e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.7 billion and increased 20%\n",
    "·         Operating income was $22.2 billion and increased 24%\n",
    "·         Net income was $18.8 billion and increased 21%\n",
    "·         Diluted earnings per share was $2.48 and increased 22%\n",
    "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
    "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
    "\n",
    "doc = nlp(earnings_text)\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"PUNCT\", \"X\"]:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2d4b9e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Microsoft,\n",
       " Corp.,\n",
       " today,\n",
       " announced,\n",
       " the,\n",
       " following,\n",
       " results,\n",
       " for,\n",
       " the,\n",
       " quarter]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "08da19f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 13,\n",
       " 92: 46,\n",
       " 100: 24,\n",
       " 90: 9,\n",
       " 85: 16,\n",
       " 93: 16,\n",
       " 97: 27,\n",
       " 98: 1,\n",
       " 84: 20,\n",
       " 103: 10,\n",
       " 87: 6,\n",
       " 99: 5,\n",
       " 89: 12,\n",
       " 86: 3,\n",
       " 94: 3,\n",
       " 95: 2}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1842f9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[96].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dc1ae889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 13\n",
      "NOUN | 46\n",
      "VERB | 24\n",
      "DET | 9\n",
      "ADP | 16\n",
      "NUM | 16\n",
      "PUNCT | 27\n",
      "SCONJ | 1\n",
      "ADJ | 20\n",
      "SPACE | 10\n",
      "AUX | 6\n",
      "SYM | 5\n",
      "CCONJ | 12\n",
      "ADV | 3\n",
      "PART | 3\n",
      "PRON | 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text, \"|\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6af8b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ab1282c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inflation rose again in April, continuing a climb that has pushed consumers to the brink and is threatening the economic expansion, the Bureau of Labor Statistics reported Wednesday.\\n\\nThe consumer price index, a broad-based measure of prices for goods and services, increased 8.3% from a year ago, higher than the Dow Jones estimate for an 8.1% gain. That represented a slight ease from Marchâ€™s peak but was still close to the highest level since the summer of 1982.\\n\\nRemoving volatile food and ene'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"news_story.txt\",\"r\") as f:\n",
    "    news_text = f.read()\n",
    "    \n",
    "news_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2678a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(news_text)\n",
    "\n",
    "numeral_tokens = []\n",
    "noun_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        noun_tokens.append(token)\n",
    "    elif token.pos_ == 'NUM':\n",
    "        numeral_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "61bd940e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.3, 8.1, 1982, 6.2, 6, â€, 0.3, 0.2, 0.6, 0.4]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeral_tokens[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "109c9cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Inflation,\n",
       " climb,\n",
       " consumers,\n",
       " brink,\n",
       " expansion,\n",
       " consumer,\n",
       " price,\n",
       " index,\n",
       " measure,\n",
       " prices]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_tokens[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6867e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{92: 98,\n",
       " 100: 27,\n",
       " 86: 15,\n",
       " 85: 39,\n",
       " 96: 17,\n",
       " 97: 32,\n",
       " 90: 34,\n",
       " 95: 4,\n",
       " 87: 13,\n",
       " 89: 10,\n",
       " 84: 23,\n",
       " 103: 7,\n",
       " 93: 20,\n",
       " 94: 4,\n",
       " 98: 8,\n",
       " 101: 1}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4d02badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN | 98\n",
      "VERB | 27\n",
      "ADV | 15\n",
      "ADP | 39\n",
      "PROPN | 17\n",
      "PUNCT | 32\n",
      "DET | 34\n",
      "PRON | 4\n",
      "AUX | 13\n",
      "CCONJ | 10\n",
      "ADJ | 23\n",
      "SPACE | 7\n",
      "NUM | 20\n",
      "PART | 4\n",
      "SCONJ | 8\n",
      "X | 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text, \"|\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9e390",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4ef7cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "43454091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "740b08e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "237af16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Bloomberg | PERSON | People, including fictional\n",
      "Bloomberg | PERSON | People, including fictional\n",
      "1982 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Michael Bloomberg founded Bloomberg in 1982\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d117530",
   "metadata": {},
   "source": [
    "Above it made a mistake in identifying Bloomberg the company. Let's try hugging face for this now.\n",
    "\n",
    "https://huggingface.co/dslim/bert-base-NER?text=Michael+Bloomberg+founded+Bloomberg+in+1982\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c7960f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  0 | 9\n",
      "Twitter Inc  |  ORG  |  30 | 41\n",
      "$45 billion  |  MONEY  |  46 | 57\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", ent.start_char, \"|\", ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "487e1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  PRODUCT\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla is going to acquire Twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2ad65",
   "metadata": {},
   "source": [
    "# Setting custom entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dc1a565d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "going to acquire"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = doc[2:5]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "18aa847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label=\"ORG\")\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")\n",
    "\n",
    "doc.set_ents([s1, s2], default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d1c23415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "077a2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #creating an object and loading the pre-trained model for \"English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "96c98bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographical location Names:  [India, Delhi, Gujarat, Tamilnadu, Pongal, Andhrapradesh, Assam, Bihar]\n",
      "Count:  8\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "          \n",
    "#list for storing all the names\n",
    "all_gpe_names = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "  if ent.label_ == 'GPE':     #checking the whether token belongs to entity \"GPE\" [Geographical location]\n",
    "    all_gpe_names.append(ent)\n",
    "\n",
    "\n",
    "\n",
    "#finally printing the results\n",
    "print(\"Geographical location Names: \", all_gpe_names)\n",
    "print(\"Count: \", len(all_gpe_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "132d73a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Birth Dates:  [24 April 1973, 5 November 1988, 7 July 1981, 19 December 1974]\n",
      "Count:  4\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "#list for storing all the dates\n",
    "all_birth_dates = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "  if ent.label_ == 'DATE':     #checking the whether token belongs to entity \"DATE\" [Dates]\n",
    "    all_birth_dates.append(ent)\n",
    "\n",
    "\n",
    "\n",
    "#finally printing the results\n",
    "print(\"All Birth Dates: \", all_birth_dates)\n",
    "print(\"Count: \", len(all_birth_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef0a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
